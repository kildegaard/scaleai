{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm new to Python, so I need you to help me write a function that will make running my Dungeons and Dragons encounters go more smoothly. As a dungeon master, I need to be able to do many different types of dice rolls, with modifiers added, quickly. The less mental math I have to do on my end the better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response 2 is definitely a better response than Response 1. Both Response 1 and Response 2 provide valid function that perform the desired action, so the model in both cases correctly reasoned out the goal for this prompt. However, Response 2 definitely took the context more into account. This is notable by referring to 20-sided die as a \"d20\", and including a print statement within the function. As a DM, sometimes you will need to roll a single d20 for 10 different NPC's at a time. This print statement would allow you to run the function one time and view each result separately. This would increase encounter smoothness specifically by making it easy for the DM to evaluate saves individually from one function run, rather than having to do several. This is in line with the goal and context provided within the prompt, making Response 2 a much more useful/preferable response compared to its counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This will help with encounters very much. Now, using the proposed roll_dice function, can you create a class that would allow me to quickly generate characters with random ability scores? Don't worry about racial or class modifiers. I'm only concerned with base character stats at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello there. I've reviewed your attempt and have some things to discuss.\n",
    "I will split my feedback in two: first I will explain to you what issues I found concerning the prompt, and then about the justification.\n",
    "\n",
    "# Prompt writing\n",
    "\n",
    "According to our updated Guidelines, there are several important factors to consider Prompt as a \"Stellar Prompt\":\n",
    "- Context: There's no context in your prompt. You should always provide one for the Model to better assess the situation. You don't describe how you want things to be done, nor any template for the Model to follow.\n",
    "- Well-defined Objective: The objective is vaguely described, but it's present implicitly within the text. It would be preferable to develop it a bit further.\n",
    "- Uniqueness: Despite the idea being pretty cool, there are dozens of apps that do this exact same thing around the internet.\n",
    "- Complexity: This prompt is not complex at all. It does not challenge the Model to improve itself when dealing with hard topics.\n",
    "- Testable: It is testable, no Issues with this.\n",
    "- Useful IRL: Yes. But again, it is a very common application easily found on many web pages.\n",
    "- No AI-Text: Agreed.\n",
    "- Misc: -\n",
    "\n",
    "\n",
    "# Justification Issues\n",
    "\n",
    "In order for your justifications to be good, they must have the following syntax:\n",
    "    [1] Comparison between responses + [2] Along which dimension? + [3] Epic quality justification + [4] Evidence for taking the aforementioned decision.\n",
    "Also, you must explain and demonstrate HOW did you test the code [5], and how (briefly describe the environment). Extra points if you can define how to fix issues (if any).\n",
    "If you follow these instructions, you will almost always be rated 4 or 5.\n",
    "\n",
    "In your specific situation, you deal well with [1, 3], but struggle with the rest. In fact, and in my opinion, there's no deviation in this case, as you rated the responses with a 7. You chose Relevance & Completeness, but both responses followed the prompt instructions while writing truthful non-code sections. At the least, I would have selected a 6 in this case, explaining that both responses completely answer the Prompt but response 2 has an extra print that adds more information to the final user.\n",
    "\n",
    "The second justification has a better syntax than the first one. But please, consider answering every single step in the justification for it to have a perfect format.\n",
    "\n",
    "With all this being said, please redo the task and follow the instructions to improve the attempt's quality to the project's required level.\n",
    "I will share with you some documentation that may come in handy to better assess this.\n",
    "\n",
    "Thanks a lot for reading till the end! I wish you the best on this, I know you have what is needed for the project!\n",
    "\n",
    "* Documentation:\n",
    "\n",
    "* Flamingo Crash Course:\n",
    "https://docs.google.com/document/d/1djY7NcldjU21bRYCX6hoFHrrQSPQ5C_o0d_XDJnNUmY/edit#heading=h.3ibr2go7c4fs\n",
    "\n",
    "* Dimension Priorities:\n",
    "https://docs.google.com/document/d/1XtlJbL3WuvMBqKvqSmRHVCujy_fvDrluiKOzR5I0qO4/edit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
