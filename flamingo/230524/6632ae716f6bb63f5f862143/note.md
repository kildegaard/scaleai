# Prompt 1

I'm a data analyst, and very often I have to work with CSV files that have bad column names.
I want you to write a **Python** CLI application that will help me easily rename CSV columns.

The code should follow these requirements:

1. Create a Python script called 'rename-csv-columns.py' that will be used to run as a command in the terminal.
2. Create a function called main with two parameters called arg_values and arg_count.
3. The application has two arguments: one is the CSV full path name, and the other is the new column names, comma-separated and enclosed in double quotes.
4. Changing column names should be done on the original file.
5. Optional flags can be used if the new column names argument was not provided. flags: -C capitalization (title case), -c all capital letters, -s all small letters.

# Justif 1

Response 1 is better than Response 2 in terms of relevance and completeness because the code generated by Response 1 met all the requirements. Response 2 on the other hand, followed all the requirements except for the new column names, which handle it as a flag (option) and not an argument.

The codes were tested using VS Code and executed with no issues, but Response 2 did not implement all the features based on the requirements.


# Prompt 2

Implement a new feature that will rename the columns based on the header columns of other CSV files. The -f flag should receive this file.

In addition, add a long version of all flags:
1. -C is --capitalize
2. -c is --capital-letters
3. -s is --small-letters
4. -f is --file

# Justif 2 original

There is no deviation because both responses generated code that was able to run without issue. Response 2 just has a little bit of an advantage in terms of explaining the code.

# Justif 2 modificada

There is no deviation because both responses generated code that was able to run locally without any issues. Response 2 just has a little bit of an advantage in terms of explaining the code, that's why response 2 is hardly better than response 1 in terms of formatting.


# Feedback

Hello, dear Tasker. Thanks a lot for your commitment to the actual project.
I reviewed your work and wanted to share my thoughts.

According to actual guidelines, your prompts are very good in terms of complexity and context, but have some minor issues to deal with; your justifications are good too, but lack some compulsory elements for them to be excellent ones.

Your prompts need to have a certain structure considering the following:
- Context: giving some environmental information will buff your prompt as the model has information to be more specific and clever.
- Clear objectives: well-explained goals for the code or problem to achieve.
- Use cases: propose uses for the code to be useful within specific situations
- Constraints: adding extra spice to the model is a good way to increase its quality. Think about specific limitations to take into account or restrictions that would make the problem non-trivial.
- Uniqueness: as stated previously, your problem must provide some fresh ideas to the prompt pool. It's expected for you to write problems that are not so easily found on the internet.
- Complexity: Once again, as stated before, the problem must not be trivial. It would be best if you aimed for issues that require some effort for the model, that provide a clear yet not easy way for solving them.

In your case, the first prompt was very good but could have more complexity introducing some constraints to it. It's a good example, it's very useful in real life and also testable, which is a very important thing too.

Another topic to engage in is justifications: in the documentation, there's plenty of information to learn some workflow to tackle this. I'll add them at the end for you to later consults.

Your justification also lacks development in it. Some things can be done to improve it further. For example, it is important to show evidence in the responses to support your response selection. You must also claim where's the deviation (in case there is), or analyze why there is not.

Another thing that is of extreme importance is to test your code factually (as stated before). Not only test it, but show some evidence that you did, and share the environment set for this. It's important because if the reviewer can not rut it, or if it's too complicated and time-consuming to set the correct environment, then your task is most probably going to be sent back to you. Please consider this!

Keynote for future justifications: I will show you a systematic way of building excellent-quality justifications.

Comparison between responses + Along which dimension? + Epic quality justification + Evidence for taking the aforementioned decision.
Also, you must explain and demonstrate HOW did you test the code, and how (briefly describe the environment). Extra points if you can define how to fix issues (if any).
If you follow these instructions, you will almost always be rated 4 or 5.


With all this, I truly believe you can develop perfect stellar prompts that would make the model thrive to create astonishing responses.

Thanks a lot for reading till the end! I wish you the best on this, I know you have what is needed for the project!

* Documentation:

* Flamingo Crash Course:
https://docs.google.com/document/d/1djY7NcldjU21bRYCX6hoFHrrQSPQ5C_o0d_XDJnNUmY/edit#heading=h.3ibr2go7c4fs

* Dimension Priorities:
https://docs.google.com/document/d/1XtlJbL3WuvMBqKvqSmRHVCujy_fvDrluiKOzR5I0qO4/edit